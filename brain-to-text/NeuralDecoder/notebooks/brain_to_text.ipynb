{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "M3qLxUaLIL1Q",
        "oRYy4X1FIZ7s",
        "arbTy1RCJUYO",
        "iT9c7hl0JYrN",
        "svCseFo0sEvY"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prep"
      ],
      "metadata": {
        "id": "M3qLxUaLIL1Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "F3Y9oRpjHcEH"
      },
      "outputs": [],
      "source": [
        "!mkdir brain-to-text/\n",
        "!git clone https://github.com/fwillett/speechBCI.git\n",
        "!pip install -q condacolab\n",
        "\n",
        "# for colab\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "\n",
        "!conda env create -f /content/speechBCI_2024/environment.yml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "exec bash"
      ],
      "metadata": {
        "id": "96GW2UHFIH1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "eval \"$(conda shell.bash hook)\"\n",
        "conda activate speech-BCI"
      ],
      "metadata": {
        "id": "4hsBAy-TIIId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e speechBCI/NeuralDecoder\n",
        "!pip install g2p-en"
      ],
      "metadata": {
        "id": "fIaCZMCEHfeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from g2p_en import G2p\n",
        "import scipy\n",
        "import pickle\n"
      ],
      "metadata": {
        "id": "GWRAyKepIT7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Decoder - Pretraining"
      ],
      "metadata": {
        "id": "oRYy4X1FIZ7s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prep Train Data"
      ],
      "metadata": {
        "id": "arbTy1RCJUYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataDir = '/content/drive/MyDrive/coding for fun/data/competitionData'\n",
        "saveDir = '/content/drive/MyDrive/TreeHacks/ptDecoder_ctc'"
      ],
      "metadata": {
        "id": "hKh5qv0RJM-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sessionNames = ['t12.2022.04.28',  't12.2022.05.26',  't12.2022.06.21',  't12.2022.07.21',  't12.2022.08.13',\n",
        "'t12.2022.05.05',  't12.2022.06.02',  't12.2022.06.23',  't12.2022.07.27',  't12.2022.08.18',\n",
        "'t12.2022.05.17',  't12.2022.06.07',  't12.2022.06.28',  't12.2022.07.29',  't12.2022.08.23',\n",
        "'t12.2022.05.19',  't12.2022.06.14',  't12.2022.07.05',  't12.2022.08.02',  't12.2022.08.25',\n",
        "'t12.2022.05.24',  't12.2022.06.16',  't12.2022.07.14',  't12.2022.08.11']\n",
        "sessionNames.sort()\n",
        "\n",
        "\n",
        "g2p = G2p()\n",
        "PHONE_DEF = [\n",
        "    'AA', 'AE', 'AH', 'AO', 'AW',\n",
        "    'AY', 'B',  'CH', 'D', 'DH',\n",
        "    'EH', 'ER', 'EY', 'F', 'G',\n",
        "    'HH', 'IH', 'IY', 'JH', 'K',\n",
        "    'L', 'M', 'N', 'NG', 'OW',\n",
        "    'OY', 'P', 'R', 'S', 'SH',\n",
        "    'T', 'TH', 'UH', 'UW', 'V',\n",
        "    'W', 'Y', 'Z', 'ZH'\n",
        "]\n",
        "PHONE_DEF_SIL = PHONE_DEF + ['SIL']\n",
        "\n",
        "def phoneToId(p):\n",
        "    return PHONE_DEF_SIL.index(p)\n",
        "\n",
        "import scipy\n",
        "\n",
        "def loadFeaturesAndNormalize(sessionPath,type='train'):\n",
        "\n",
        "    dat = scipy.io.loadmat(sessionPath)\n",
        "\n",
        "    input_features = []\n",
        "    transcriptions = []\n",
        "    frame_lens = []\n",
        "    block_means = []\n",
        "    block_stds = []\n",
        "    n_trials = dat['sentenceText'].shape[0]\n",
        "\n",
        "    #collect area 6v tx1 and spikePow features\n",
        "    for i in range(n_trials):\n",
        "        #get time series of TX and spike power for this trial\n",
        "        #first 128 columns = area 6v only\n",
        "        if type == 'train':\n",
        "            features = np.concatenate([dat['tx1'][0,i][:,0:128], dat['spikePow'][0,i][:,0:128]], axis=1)\n",
        "        else:\n",
        "            features = dat['data']\n",
        "\n",
        "        sentence_len = features.shape[0]\n",
        "        sentence = dat['sentenceText'][i].strip()\n",
        "\n",
        "        input_features.append(features)\n",
        "        transcriptions.append(sentence)\n",
        "        frame_lens.append(sentence_len)\n",
        "\n",
        "    #block-wise feature normalization\n",
        "    blockNums = np.squeeze(dat['blockIdx'])\n",
        "    blockList = np.unique(blockNums)\n",
        "    blocks = []\n",
        "    for b in range(len(blockList)):\n",
        "        sentIdx = np.argwhere(blockNums==blockList[b])\n",
        "        sentIdx = sentIdx[:,0].astype(np.int32)\n",
        "        blocks.append(sentIdx)\n",
        "\n",
        "    for b in range(len(blocks)):\n",
        "        feats = np.concatenate(input_features[blocks[b][0]:(blocks[b][-1]+1)], axis=0)\n",
        "        feats_mean = np.mean(feats, axis=0, keepdims=True)\n",
        "        feats_std = np.std(feats, axis=0, keepdims=True)\n",
        "        for i in blocks[b]:\n",
        "            input_features[i] = (input_features[i] - feats_mean) / (feats_std + 1e-8)\n",
        "\n",
        "    #convert to tfRecord file\n",
        "    session_data = {\n",
        "        'inputFeatures': input_features,\n",
        "        'transcriptions': transcriptions,\n",
        "        'frameLens': frame_lens\n",
        "    }\n",
        "\n",
        "    return session_data\n",
        "\n",
        "\n",
        "def getDataset(fileName,type='train'):\n",
        "    session_data = loadFeaturesAndNormalize(fileName,type)\n",
        "\n",
        "    allDat = []\n",
        "    trueSentences = []\n",
        "    seqElements = []\n",
        "\n",
        "    for x in range(len(session_data['inputFeatures'])):\n",
        "        allDat.append(session_data['inputFeatures'][x])\n",
        "        trueSentences.append(session_data['transcriptions'][x])\n",
        "\n",
        "        thisTranscription = str(session_data['transcriptions'][x]).strip()\n",
        "        thisTranscription = re.sub(r'[^a-zA-Z\\- \\']', '', thisTranscription)\n",
        "        thisTranscription = thisTranscription.replace('--', '').lower()\n",
        "        addInterWordSymbol = True\n",
        "\n",
        "        phonemes = []\n",
        "        for p in g2p(thisTranscription):\n",
        "            if addInterWordSymbol and p==' ':\n",
        "                phonemes.append('SIL')\n",
        "            p = re.sub(r'[0-9]', '', p)  # Remove stress\n",
        "            if re.match(r'[A-Z]+', p):  # Only keep phonemes\n",
        "                phonemes.append(p)\n",
        "\n",
        "        #add one SIL symbol at the end so there's one at the end of each word\n",
        "        if addInterWordSymbol:\n",
        "            phonemes.append('SIL')\n",
        "\n",
        "        seqLen = len(phonemes)\n",
        "        maxSeqLen = 500\n",
        "        seqClassIDs = np.zeros([maxSeqLen]).astype(np.int32)\n",
        "        seqClassIDs[0:seqLen] = [phoneToId(p) + 1 for p in phonemes]\n",
        "        seqElements.append(seqClassIDs)\n",
        "\n",
        "    newDataset = {}\n",
        "    newDataset['sentenceDat'] = allDat\n",
        "    newDataset['transcriptions'] = trueSentences\n",
        "    newDataset['phonemes'] = seqElements\n",
        "\n",
        "    timeSeriesLens = []\n",
        "    phoneLens = []\n",
        "    for x in range(len(newDataset['sentenceDat'])):\n",
        "        timeSeriesLens.append(newDataset['sentenceDat'][x].shape[0])\n",
        "\n",
        "        zeroIdx = np.argwhere(newDataset['phonemes'][x]==0)\n",
        "        phoneLens.append(zeroIdx[0,0])\n",
        "\n",
        "    newDataset['timeSeriesLens'] = np.array(timeSeriesLens)\n",
        "    newDataset['phoneLens'] = np.array(phoneLens)\n",
        "    newDataset['phonePerTime'] = newDataset['phoneLens'].astype(np.float32) / newDataset['timeSeriesLens'].astype(np.float32)\n",
        "    return newDataset\n",
        "\n",
        "trainDatasets = []\n",
        "testDatasets = []\n",
        "competitionDatasets = []\n",
        "\n",
        "\n",
        "for dayIdx in range(len(sessionNames)):\n",
        "    print(dayIdx)\n",
        "    trainDataset = getDataset(dataDir + '/train/' + sessionNames[dayIdx] + '.mat')\n",
        "    testDataset = getDataset(dataDir + '/test/' + sessionNames[dayIdx] + '.mat')\n",
        "\n",
        "    trainDatasets.append(trainDataset)\n",
        "    testDatasets.append(testDataset)\n",
        "\n",
        "    if os.path.exists(dataDir + '/competitionHoldOut/' + sessionNames[dayIdx] + '.mat'):\n",
        "        dataset = getDataset(dataDir + '/competitionHoldOut/' + sessionNames[dayIdx] + '.mat')\n",
        "        competitionDatasets.append(dataset)"
      ],
      "metadata": {
        "id": "0ZHDW8_nIcEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "competitionDays = []\n",
        "for dayIdx in range(len(sessionNames)):\n",
        "    if os.path.exists(dataDir + '/competitionHoldOut/' + sessionNames[dayIdx] + '.mat'):\n",
        "        competitionDays.append(dayIdx)\n",
        "print(competitionDays)\n",
        "\n",
        "\n",
        "allDatasets = {}\n",
        "allDatasets['train'] = trainDatasets\n",
        "allDatasets['test'] = testDatasets\n",
        "allDatasets['competition'] = competitionDatasets\n",
        "\n",
        "\n",
        "with open(saveDir, 'wb') as handle:\n",
        "    pickle.dump(allDatasets, handle)"
      ],
      "metadata": {
        "id": "nN2Gg1YYJGJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build & Train Model"
      ],
      "metadata": {
        "id": "iT9c7hl0JYrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelSaveDir = '/content/drive/MyDrive/TreeHacks/NeuralDecoderModel'"
      ],
      "metadata": {
        "id": "cXFphs1MJmnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/\")\n",
        "!git clone https://github.com/cffan/neural_seq_decoder.git\n",
        "os.chdir(\"/content/neural_seq_decoder/src/\")\n",
        "!pip install -e .."
      ],
      "metadata": {
        "id": "BmO6Yo8IJaqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "modelName = 'speechBaseline4'\n",
        "\n",
        "args = {}\n",
        "args['outputDir'] = modelSaveDir\n",
        "args['datasetPath'] = saveDir\n",
        "args['seqLen'] = 150\n",
        "args['maxTimeSeriesLen'] = 1200\n",
        "args['batchSize'] = 64\n",
        "args['lrStart'] = 0.02\n",
        "args['lrEnd'] = 0.02\n",
        "args['nUnits'] = 1024\n",
        "args['nBatch'] = 10000\n",
        "args['nLayers'] = 5\n",
        "args['seed'] = 0\n",
        "args['nClasses'] = 40\n",
        "args['nInputFeatures'] = 256\n",
        "args['dropout'] = 0.4\n",
        "args['whiteNoiseSD'] = 0.8\n",
        "args['constantOffsetSD'] = 0.2\n",
        "args['gaussianSmoothWidth'] = 2.0\n",
        "args['strideLen'] = 4\n",
        "args['kernelLen'] = 32\n",
        "args['bidirectional'] = True\n",
        "args['l2_decay'] = 1e-5\n",
        "\n",
        "from neural_decoder.neural_decoder_trainer import trainModel, getDatasetLoaders\n",
        "\n",
        "model=trainModel(args)"
      ],
      "metadata": {
        "id": "NunhDPQqJhu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Decoder - Fine-Tuning"
      ],
      "metadata": {
        "id": "6-_InkmJJ18F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prep Data"
      ],
      "metadata": {
        "id": "SpFo7rc4J7dL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuning_dataset_path = \"/content/fine-tuning-data.csv\""
      ],
      "metadata": {
        "id": "0NsVWYIkKdDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bci_data = pd.read_csv(fine_tuning_dataset_path)\n",
        "\n",
        "def inflate_dims(arr, group_size=16):\n",
        "\n",
        "    x, y = arr.shape\n",
        "    mid = y // 2\n",
        "\n",
        "    first_half = arr[:, :mid]\n",
        "    second_half = arr[:, mid:]\n",
        "\n",
        "    result = np.concatenate([\n",
        "        np.repeat(first_half, group_size, axis=1),\n",
        "        np.repeat(second_half, group_size, axis=1)\n",
        "    ], axis=1)\n",
        "\n",
        "    return result\n",
        "\n",
        "dataset = {\n",
        "        'data': inflate_dims(bci_data.T.values),\n",
        "        'sentenceText': np.array(['hello there',]),\n",
        "        'blockIdx': np.arange(0).repeat(20).reshape(-1,1)\n",
        "    }\n",
        "\n",
        "scipy.io.savemat(fine_tuning_dataset_path, dataset)\n",
        "\n",
        "fine_tune_dataset=getDataset(fine_tuning_dataset_path,type='val')\n",
        "\n",
        "# temporary fix\n",
        "d = {\"train\":[fine_tune_dataset],'test':[fine_tune_dataset]}\n",
        "\n",
        "with open('/content/local_tmp', 'wb') as handle:\n",
        "    pickle.dump(d, handle)"
      ],
      "metadata": {
        "id": "D3rfryCGJ6NU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FT"
      ],
      "metadata": {
        "id": "svCseFo0sEvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class SpeechDataset(Dataset):\n",
        "    def __init__(self, data, transform=None):\n",
        "        self.data = data\n",
        "        self.transform = transform\n",
        "        self.n_days = len(data)\n",
        "        self.n_trials = sum([len(d[\"sentenceDat\"]) for d in data])\n",
        "\n",
        "        self.neural_feats = []\n",
        "        self.phone_seqs = []\n",
        "        self.neural_time_bins = []\n",
        "        self.phone_seq_lens = []\n",
        "        self.days = []\n",
        "        for day in range(self.n_days):\n",
        "            for trial in range(len(data[day][\"sentenceDat\"])):\n",
        "                self.neural_feats.append(data[day][\"sentenceDat\"][trial])\n",
        "                self.phone_seqs.append(data[day][\"phonemes\"][trial])\n",
        "                self.neural_time_bins.append(data[day][\"sentenceDat\"][trial].shape[0])\n",
        "                self.phone_seq_lens.append(data[day][\"phoneLens\"][trial])\n",
        "                self.days.append(day)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_trials\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        neural_feats = torch.tensor(self.neural_feats[idx], dtype=torch.float32)\n",
        "\n",
        "        if self.transform:\n",
        "            neural_feats = self.transform(neural_feats)\n",
        "\n",
        "        return (\n",
        "            neural_feats,\n",
        "            torch.tensor(self.phone_seqs[idx], dtype=torch.int32),\n",
        "            torch.tensor(self.neural_time_bins[idx], dtype=torch.int32),\n",
        "            torch.tensor(self.phone_seq_lens[idx], dtype=torch.int32),\n",
        "            torch.tensor(self.days[idx], dtype=torch.int64),\n",
        "        )\n"
      ],
      "metadata": {
        "id": "OnDoGuGoLLKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def prepare_fine_tune_data(session_path: str):\n",
        "\n",
        "    dataset = getDataset(session_path, 'val')\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def fine_tune_model(\n",
        "    model: torch.nn.Module,\n",
        "    session_path: str,\n",
        "    config: Optional[Dict] = None,\n",
        "    device: str = \"cuda\"\n",
        ")\n",
        "    default_config = {\n",
        "        \"learning_rate\": 1e-4,\n",
        "        \"num_epochs\": 10,\n",
        "        \"batch_size\": 32,\n",
        "        \"weight_decay\": 0.01,\n",
        "        \"freeze_layers\": [],\n",
        "        \"gradient_clip\": 1.0\n",
        "    }\n",
        "\n",
        "\n",
        "    if config is not None:\n",
        "        default_config.update(config)\n",
        "    config = default_config\n",
        "\n",
        "    fine_tune_data = prepare_fine_tune_data(session_path)\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    if config[\"freeze_layers\"]:\n",
        "        for name, param in model.named_parameters():\n",
        "            if any(layer in name for layer in config[\"freeze_layers\"]):\n",
        "                param.requires_grad = False\n",
        "\n",
        "\n",
        "    dataloader, _, _ = getDatasetLoaders(\n",
        "        '/content/local_tmp',\n",
        "        args[\"batchSize\"]\n",
        "    )\n",
        "\n",
        "    criterion = torch.nn.CTCLoss(blank=0, reduction=\"mean\", zero_infinity=True)\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        filter(lambda p: p.requires_grad, model.parameters()),\n",
        "        lr=config[\"learning_rate\"],\n",
        "        weight_decay=config[\"weight_decay\"]\n",
        "    )\n",
        "\n",
        "    model.train()\n",
        "    best_loss = float('inf')\n",
        "    training_stats = {\n",
        "        \"epoch_losses\": [],\n",
        "        \"best_epoch\": 0,\n",
        "        \"final_phone_per_time\": float(fine_tune_data['phonePerTime'].mean())\n",
        "    }\n",
        "\n",
        "    for epoch in range(config[\"num_epochs\"]):\n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        for features, phonemes, time_lens, phone_lens, indices in dataloader:\n",
        "            # Move batch to device\n",
        "            features = features.to(device)\n",
        "            phonemes = phonemes.to(device)\n",
        "            time_lens = time_lens.to(device)\n",
        "            phone_lens = phone_lens.to(device)\n",
        "            indices = indices.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            pred = model(features, indices)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(\n",
        "                torch.permute(pred.log_softmax(2), [1, 0, 2]),\n",
        "                phonemes,\n",
        "                ((time_lens - model.kernelLen) / model.strideLen).to(torch.int32),\n",
        "                phone_lens\n",
        "            )\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                model.parameters(),\n",
        "                config[\"gradient_clip\"]\n",
        "            )\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "        avg_epoch_loss = epoch_loss / num_batches\n",
        "        training_stats[\"epoch_losses\"].append(avg_epoch_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{config['num_epochs']}, \"\n",
        "              f\"Average Loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "        if avg_epoch_loss < best_loss:\n",
        "            best_loss = avg_epoch_loss\n",
        "            best_state = model.state_dict()\n",
        "            training_stats[\"best_epoch\"] = epoch\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "\n",
        "    return model, training_stats\n",
        "\n",
        "# temp\n",
        "config = {\n",
        "    \"dtype\": 'float'\n",
        "}\n"
      ],
      "metadata": {
        "id": "6DJBKhvbLW-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FTModelSaveDir = \"/content/drive/MyDrive/TreeHacks/FTModel\""
      ],
      "metadata": {
        "id": "FaUiMJWFNP1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune the model\n",
        "fine_tuned_model, stats = fine_tune_model(\n",
        "    model=model,\n",
        "    session_path=fine_tuning_dataset_path,\n",
        "    config=config\n",
        ")\n",
        "\n",
        "torch.save(fine_tuned_model.state_dict(), FTModelSaveDir + \"/modelWeights\")"
      ],
      "metadata": {
        "id": "kbB_qz_2M7oj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}